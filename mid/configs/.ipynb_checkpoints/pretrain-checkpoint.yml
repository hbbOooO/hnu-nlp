processors:
- id: IdProcessor
  class_name: IdProcessor
  module_path: common.processors.base_processor
  config:
- id: MidProcessor
  class_name: MidProcessorWOPad
  module_path: mid.processors.mid_processor
  config:
    path: /root/autodl-tmp/data/mid/pretrained_bert/vocab.txt
    max_input_length: 150
    max_target_length: 80
    label_pad_id: -100
    pad_id: 0

dataset:
  class_name: PretrainDataset
  module_path: mid.datasets.pretrain_dataset
  datasets:
  - dataset_type: train
    data_root_dir: /root/autodl-tmp/data/mid/pretrain_data/
    data_file_path: 
    - document_rotation_train.json
    - sentence_permutation_train.json
    - text_infilling_train.json
    - token_deletion_train.json
    - token_masking_train.json
    processor_ids:
    - IdProcessor
    - MidProcessor
  - dataset_type: val
    data_root_dir: /root/autodl-tmp/data/mid/pretrain_data/
    data_file_path: 
    - document_rotation_val.json
    - sentence_permutation_val.json
    - text_infilling_val.json
    - token_deletion_val.json
    - token_masking_val.json
    processor_ids:
    - IdProcessor
    - MidProcessor
    
model:
  class_name: PretrainModel
  module_path: mid.models.pretrain_model
  model:
    auto_net:
      auto_net_path: fnlp/bart-base-chinese
      auto_net_config: {}
    vocab_size: 1415
    max_length: 80
    num_beams: 5
    synced_gpus: false
run_param:
  run_type: train
  log_dir: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/logs/
  log_level: info
  timer_type: all
  cuda_idx: 0
  collate_fn: mid_collate
  resume_file: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/model/epoch_best_9069.ckpt
  meter:
    module_path: common.meters
    class_name: RougeMeter
  train_param:
    max_epoch: 50
    batch_size: 50
    log_interval: 200
    val_on_val_set: true     # 验证集上测试
    val_on_train_set: false   # 训练集上测试
    optimizer:
      optimizer_name: adamw  # 原来使用的是adamw优化器，并且还有权重衰减。并且一部分有衰减，另一部分没有衰减，这个功能没有实现
      optimizer_config:
        weight_decay: 0.001
      use_warmup: true
      warmup_iter: 500
      lr: 0.0002
      lr_strategy: linear
      start_epoch: 1
      end_epoch: 20
      min_lr: 0.00001
      # lr_strategy: step
      # lr_step:
      # - 11
      # - 15
    loss:
      loss_name: LabelSmootherLoss
      label_smoothing_factor: 0.1
    checkpoint:
      ckpt_dir: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/model/
      save_by_epoch: false
      save_interval: 1
      save_best: true
      save_last: false
    module_save_path: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/pretrained_bart/
  val_param:
    batch_size: 50
  inference_param:
    batch_size: 1
    out_path: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/out/prediction.jsonl
processors:
- id: IdProcessor
  class_name: IdProcessor
  module_path: common.processors.base_processor
  config:
- id: MidProcessor
  class_name: MidProcessor
  module_path: mid.processors.mid_processor
  config:
    path: /root/autodl-tmp/data/mid/pretrained_bert/vocab.txt
    max_input_length: 150
    max_target_length: 80

dataset:
  class_name: PretrainStage2Dataset
  module_path: mid.datasets.pretrain_dataset
  datasets:
  - dataset_type: train
    data_root_dir: /root/autodl-tmp/data/mid/stage2/pretrain_data/
    data_file_path: 
    - document_rotation_train.csv
    - sentence_permutation_train.csv
    - text_infilling_train.csv
    - token_deletion_train.csv
    - token_masking_train.csv
    processor_ids:
    - IdProcessor
    - MidProcessor
  - dataset_type: val
    data_root_dir: /root/autodl-tmp/data/mid/stage2/pretrain_data/
    data_file_path: 
    - document_rotation_val.csv
    - sentence_permutation_val.csv
    - text_infilling_val.csv
    - token_deletion_val.csv
    - token_masking_val.csv
    processor_ids:
    - IdProcessor
    - MidProcessor
    
model:
  class_name: PretrainModel
  module_path: mid.models.pretrain_model
  config:
    auto_net:
      auto_net_path: fnlp/bart-large-chinese
      auto_net_config: {}
    vocab_size: 1415
    max_length: 80
    num_beams: 5
    synced_gpus: false
run_param:
  run_type: train
  log_dir: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/stage2/pad_large/logs/
  log_level: info
  timer_type: all
  cuda_idx: 0
  resume_file: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/stage2/pad_large/model/epoch_best_9088.ckpt
  dataloader:
  - dataloader_type: train
    config:
      batch_size: 32
      shuffle: true
  meter:
    module_path: common.meters
    class_name: RougeMeter
  train_param:
    max_epoch: 50
    log_interval: 1000
    val_on_val_set: true     # 验证集上测试
    val_on_train_set: false   # 训练集上测试
    optimizer:
      optimizer_name: adamw  # 原来使用的是adamw优化器，并且还有权重衰减。并且一部分有衰减，另一部分没有衰减，这个功能没有实现
      optimizer_config:
        weight_decay: 0.001
      use_warmup: true
      warmup_iter: 500
      lr: 0.00001
      # lr_strategy: linear
      # start_epoch: 1
      # end_epoch: 20
      gradient_accumulation_steps: 1
    loss:
      class_name: LabelSmootherLoss
      module_path: common.losses
      config:
        label_smoothing_factor: 0.1
    checkpoint:
      ckpt_dir: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/stage2/pad_large/model/
      save_by_epoch: false
      save_interval: 1
      save_best: true
      save_last: false
    save_module:
      module_dir: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/stage2/pad_large/pretrained_bart/
      save_by_epoch: false
      save_interval: 1
      save_best: true
      save_last: true
  val_param: {}
  inference_param:
    out_path: /root/autodl-tmp/save/hnu-nlp/mid/pretrain/stage2/pad_large/out/prediction.jsonl